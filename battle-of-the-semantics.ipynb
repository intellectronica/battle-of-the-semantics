{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Battle of the Semantics: GraphRag vs Embeddings Index\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is often performed by chunking long texts, creating a text embedding for each chunk, and retrieving chunks for including in the LLM generation context based on a similarity search against the query. This approach works well in many scenarios, and at compelling speed and cost trade-offs, but doesn't always cope well in scenarios where a detailed understanding of the text is required.\n",
    "\n",
    "GraphRag ( [microsoft.github.io/graphrag](https://microsoft.github.io/graphrag/) ), a new indexing method released by Microsoft, promises to address this defficiency by using an LLM to analyse the indexed text and construct and knowledge graph of entities from it. This more detailed semantic understanding of the content of the text can result in searches that produces a more accurate and complete context for the LLM to work with in generation.\n",
    "\n",
    "To compare both method, let's see what results we get when indexing and retrieving the text with both techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai graphrag pandas requests python-dotenv langchain numpy tiktoken matplotlib scikit-learn pyyaml pydantic instructor\n",
    "from IPython.display import clear_output ; clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this test yourself, copy the template file `dot.env` to `.env` and fill in the details for your OpenAI or Azure Open AI endpoint. The following code loads these environment variables and sets up our AI client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "is_azure = (\n",
    "  os.getenv(\"AZURE_OPENAI_ENDPOINT\", default=\"\") != \"\" and\n",
    "  os.getenv(\"OPENAI_API_KEY\", default=\"\") == \"\"\n",
    ")\n",
    "\n",
    "GPT_4_O_MODEL_NAME = os.getenv(\"GPT_4_O_MODEL_NAME\", default=\"gpt-4o\")\n",
    "TEXT_EMBEDDING_3_LARGE_MODEL_NAME = os.getenv(\"TEXT_EMBEDDING_3_LARGE_MODEL_NAME\", default=\"text-embedding-3-large\")\n",
    "\n",
    "if is_azure:\n",
    "  AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "  AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "  AZURE_OPENAI_API_VERSION = \"2024-05-01-preview\"\n",
    "  from openai import AzureOpenAI\n",
    "  oai = AzureOpenAI(azure_endpoint=AZURE_OPENAI_ENDPOINT, api_key=AZURE_OPENAI_API_KEY, api_version=AZURE_OPENAI_API_VERSION)\n",
    "else:\n",
    "  OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "  from openai import OpenAI\n",
    "  oai = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by getting a text to work with. The Wikipedia article on the French Revolution is a longer text, rich in detail and structure. We'll download it from Wikipedia and convert it to an LLM-digestable piece of text using the Jina AI Reader API. We'll also trim the last sections of the article (See Also and References) that do not contribute relevant content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: French Revolution\n",
      "\n",
      "URL Source: https://en.wikipedia.org/wiki/French_Revolution\n",
      "\n",
      "Published Time: 2001-10-18T00:19:10Z\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "if not os.path.exists('data'): os.makedirs('data')\n",
    "\n",
    "if not os.path.exists('data/french_revolution.md'):\n",
    "  french_revolution = requests.get(\"https://r.jina.ai/https://en.wikipedia.org/wiki/French_Revolution\").text.split('\\nSee also')[0]\n",
    "  with open('data/french_revolution.md', 'w') as f:\n",
    "    f.write(french_revolution)\n",
    "else:\n",
    "  with open('data/french_revolution.md', 'r') as f:\n",
    "    french_revolution = f.read()\n",
    "\n",
    "print(french_revolution[:123])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now chunk the text and embed the chunks. An optimal chunking strategy is content-dependent, but this particular one works well for articles of this length and format, and optimising for reducing context length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Title: French Revolution\\n\\nURL Source: https:...</td>\n",
       "      <td>[-0.003698753658682108, -0.019621584564447403,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Markdown Content:\\nJump to content\\nMain menu\\...</td>\n",
       "      <td>[-0.01978607289493084, 0.009914387948811054, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tools\\nFrom Wikipedia, the free encyclopedia\\n...</td>\n",
       "      <td>[-0.032426539808511734, -0.009258555248379707,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Storming of the Bastille, 14 July 1789</td>\n",
       "      <td>[0.006268054712563753, -0.0037634416949003935,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Date\\t5 May 1789 – 9 November 1799\\n(10 years,...</td>\n",
       "      <td>[-0.014528979547321796, 0.0037560712080448866,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>Alfred Cobban challenged Jacobin-Marxist socia...</td>\n",
       "      <td>[-0.012176590040326118, -0.012197822332382202,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>Revolution (1964). He argued the Revolution wa...</td>\n",
       "      <td>[-0.0006112174596637487, 0.02068362943828106, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>In their 1965 work, La Revolution française, F...</td>\n",
       "      <td>[0.01710551045835018, 0.012524602934718132, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>From the 1990s, Western scholars largely aband...</td>\n",
       "      <td>[0.00677307927981019, 0.007338008843362331, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>gained widespread support.[232][278] The histo...</td>\n",
       "      <td>[0.02364640310406685, 0.03662174940109253, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>470 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  \\\n",
       "0    Title: French Revolution\\n\\nURL Source: https:...   \n",
       "1    Markdown Content:\\nJump to content\\nMain menu\\...   \n",
       "2    Tools\\nFrom Wikipedia, the free encyclopedia\\n...   \n",
       "3           The Storming of the Bastille, 14 July 1789   \n",
       "4    Date\\t5 May 1789 – 9 November 1799\\n(10 years,...   \n",
       "..                                                 ...   \n",
       "465  Alfred Cobban challenged Jacobin-Marxist socia...   \n",
       "466  Revolution (1964). He argued the Revolution wa...   \n",
       "467  In their 1965 work, La Revolution française, F...   \n",
       "468  From the 1990s, Western scholars largely aband...   \n",
       "469  gained widespread support.[232][278] The histo...   \n",
       "\n",
       "                                             Embedding  \n",
       "0    [-0.003698753658682108, -0.019621584564447403,...  \n",
       "1    [-0.01978607289493084, 0.009914387948811054, -...  \n",
       "2    [-0.032426539808511734, -0.009258555248379707,...  \n",
       "3    [0.006268054712563753, -0.0037634416949003935,...  \n",
       "4    [-0.014528979547321796, 0.0037560712080448866,...  \n",
       "..                                                 ...  \n",
       "465  [-0.012176590040326118, -0.012197822332382202,...  \n",
       "466  [-0.0006112174596637487, 0.02068362943828106, ...  \n",
       "467  [0.01710551045835018, 0.012524602934718132, -0...  \n",
       "468  [0.00677307927981019, 0.007338008843362331, -0...  \n",
       "469  [0.02364640310406685, 0.03662174940109253, -0....  \n",
       "\n",
       "[470 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "import pandas as pd\n",
    "\n",
    "if not os.path.exists('data/embeddings.parquet'):\n",
    "  embeddings = pd.DataFrame(columns=['Text', 'Embedding'])\n",
    "\n",
    "  splitter = MarkdownTextSplitter(chunk_size=300, chunk_overlap=100)\n",
    "\n",
    "  chunks = splitter.split_text(french_revolution)\n",
    "  chunk_embeddings = oai.embeddings.create(\n",
    "    input=chunks,\n",
    "    model=TEXT_EMBEDDING_3_LARGE_MODEL_NAME\n",
    "  )\n",
    "  for i, chunk in enumerate(chunks):\n",
    "    embeddings.loc[len(embeddings)] = [chunk, chunk_embeddings.data[i].embedding]\n",
    "  embeddings.to_parquet('data/embeddings.parquet')\n",
    "else:\n",
    "  embeddings = pd.read_parquet('data/embeddings.parquet')\n",
    "\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To search for chunks, we'll use cosine similarity, a popular comparison method for embeddings. Our search function will look for the chunks with the highest similarity index and limit the set of results by the number of tokens we're willing to use in our context for generation (for convenience and debugging, we can also filter by the top number of results or by a similarity threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tiktoken\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "  dot_product = np.dot(vector1, vector2)\n",
    "  norm1 = np.linalg.norm(vector1)\n",
    "  norm2 = np.linalg.norm(vector2)\n",
    "  similarity = dot_product / (norm1 * norm2)\n",
    "  return similarity\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model('gpt-4o')\n",
    "\n",
    "def embeddings_search(query, max_tokens=10000, k=100, min_similarity=0.2):\n",
    "  query_embedding = oai.embeddings.create(\n",
    "    input=[query],\n",
    "    model=TEXT_EMBEDDING_3_LARGE_MODEL_NAME\n",
    "  ).data[0].embedding\n",
    "  results = embeddings.copy()\n",
    "  results['Similarity'] = results['Embedding'].apply(lambda x: cosine_similarity(x, query_embedding))\n",
    "  results = results.sort_values(by='Similarity', ascending=False).head(k)\n",
    "  results = results[results['Similarity'] >= min_similarity]\n",
    "  results['Tokens'] = results['Text'].apply(lambda txt: len(tokenizer.encode(txt)))\n",
    "  while results['Tokens'].sum() > max_tokens:\n",
    "    results = results[:-1]\n",
    "  return results['Text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a complete RAG with our embeddings, we'll retrieve chunks using our embeddings search then pass those on to the LLM for generation. Note that we're using the same system prompt used in GraphRag, to make sure that our comparison only differs on the retrieved context, and not the instructions to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphrag.query.structured_search.global_search.reduce_system_prompt import REDUCE_SYSTEM_PROMPT as SYSTEM_PROMPT\n",
    "import re\n",
    "\n",
    "DEFAULT_RESPONSE_TYPE = 'Summarize and explain in 1-2 paragraphs with bullet points using at most 300 tokens'\n",
    "DEFAULT_MAX_CONTEXT_TOKENS = 10000\n",
    "\n",
    "def remove_data(text):\n",
    "    return re.sub(r'\\[Data:.*?\\]', '', text).strip()\n",
    "\n",
    "def ask_embeddings(query):\n",
    "  results = embeddings_search(query, max_tokens=DEFAULT_MAX_CONTEXT_TOKENS)\n",
    "  response = oai.chat.completions.create(\n",
    "    model=GPT_4_O_MODEL_NAME,\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": SYSTEM_PROMPT.format(\n",
    "          response_type=DEFAULT_RESPONSE_TYPE,\n",
    "          report_data=\"---\\n---\\n\".join(results),\n",
    "        ),\n",
    "      },\n",
    "      {\"role\": \"user\", \"content\": query}\n",
    "    ],\n",
    "    max_tokens=4000,\n",
    "    temperature=0.5,\n",
    "  ).choices[0].message.content\n",
    "  return remove_data(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's index using GraphRag. GraphRag has a convenient set of CLI commands we can use. We'll start by configuring the system, then run the indexing operation. Indexing with GraphRag is a much lengthier process, and one that costs significantly more, since rather than just calculating embeddings, GraphRag makes many LLM calls to analyse the text, extract entities, and construct the graph. That's a one-time expense, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "if not os.path.exists('data/graphrag'):\n",
    "  !python -m graphrag.index --init --root data/graphrag\n",
    "\n",
    "with open('data/graphrag/settings.yaml', 'r') as f:\n",
    "  settings_yaml = yaml.load(f, Loader=yaml.FullLoader)\n",
    "settings_yaml['llm']['model'] = GPT_4_O_MODEL_NAME\n",
    "settings_yaml['llm']['api_key'] = AZURE_OPENAI_API_KEY if is_azure else OPENAI_API_KEY\n",
    "settings_yaml['llm']['type'] = 'azure_openai_chat' if is_azure else 'openai_chat'\n",
    "settings_yaml['embeddings']['llm']['api_key'] = AZURE_OPENAI_API_KEY if is_azure else OPENAI_API_KEY\n",
    "settings_yaml['embeddings']['llm']['type'] = 'azure_openai_embedding' if is_azure else 'openai_embedding'\n",
    "settings_yaml['embeddings']['llm']['model'] = TEXT_EMBEDDING_3_LARGE_MODEL_NAME\n",
    "if is_azure:\n",
    "  settings_yaml['llm']['api_version'] = AZURE_OPENAI_API_VERSION\n",
    "  settings_yaml['llm']['deployment_name'] = GPT_4_O_MODEL_NAME\n",
    "  settings_yaml['llm']['api_base'] = AZURE_OPENAI_ENDPOINT\n",
    "  settings_yaml['embeddings']['llm']['api_version'] = AZURE_OPENAI_API_VERSION\n",
    "  settings_yaml['embeddings']['llm']['deployment_name'] = TEXT_EMBEDDING_3_LARGE_MODEL_NAME\n",
    "  settings_yaml['embeddings']['llm']['api_base'] = AZURE_OPENAI_ENDPOINT\n",
    "\n",
    "with open('data/graphrag/settings.yaml', 'w') as f:\n",
    "  yaml.dump(settings_yaml, f)\n",
    "\n",
    "if not os.path.exists('data/graphrag/input'):\n",
    "  os.makedirs('data/graphrag/input')\n",
    "  !cp data/french_revolution.md data/graphrag/input/french_revolution.txt\n",
    "  !python -m graphrag.index --root ./data/graphrag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To query GraphRag we'll use its CLI again, making sure to configure it with a context length equivalent to what we use in our embeddings search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def ask_graph(query):\n",
    "  env = os.environ.copy() | {\n",
    "    'GRAPHRAG_GLOBAL_SEARCH_MAX_TOKENS': str(DEFAULT_MAX_CONTEXT_TOKENS),\n",
    "  }\n",
    "  command = [\n",
    "    'python', '-m', 'graphrag.query',\n",
    "    '--root', './data/graphrag',\n",
    "    '--method', 'global',\n",
    "    '--response_type', DEFAULT_RESPONSE_TYPE,\n",
    "    query,\n",
    "  ]\n",
    "  output = subprocess.check_output(command, universal_newlines=True, env=env, stderr=subprocess.DEVNULL)\n",
    "  return remove_data(output.split('Search Response: ')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a couple of questions and compare. Is the extra cost of indexing with GraphRag worth it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Timeline of the French revolution (ask_embeddings)**\n",
       "\n",
       "### Timeline of the French Revolution\n",
       "\n",
       "The French Revolution was a period of significant political and societal change in France, spanning from May 5, 1789, to November 9, 1799. Below are the key events and outcomes:\n",
       "\n",
       "- **May 5, 1789**: The Estates General convened, marking the beginning of the Revolution.\n",
       "- **July 14, 1789**: The Storming of the Bastille, a symbolic event that is still commemorated as Bastille Day.\n",
       "- **August 10, 1792**: Insurrection led to the abolition of the monarchy.\n",
       "- **September 22, 1792**: Proclamation of the French First Republic.\n",
       "- **January 21, 1793**: Execution of Louis XVI.\n",
       "- **September 1793 - July 1794**: The Reign of Terror, during which around 16,000 people were executed.\n",
       "- **November 9, 1799**: The coup of 18 Brumaire led to the establishment of the French Consulate, marking the end of the Revolution .\n",
       "\n",
       "### Outcomes and Impact\n",
       "\n",
       "- **Abolition of the Ancien Régime**: The Revolution dismantled feudal structures and privileges.\n",
       "- **Creation of a Constitutional Monarchy**: Initially, the Revolution aimed to establish a constitutional monarchy, which later transitioned to a republic.\n",
       "- **Influence on Western History**: The Revolution ended feudalism in France and paved the way for advances in individual freedoms and democratic ideals across Europe .\n",
       "\n",
       "These events and outcomes highlight the transformative nature of the French Revolution, which continues to influence modern political discourse and democratic principles.\n",
       "\n",
       "---\n",
       "\n",
       "**Timeline of the French revolution (ask_graph)**\n",
       "\n",
       "### Timeline of the French Revolution\n",
       "\n",
       "The French Revolution, spanning from 1789 to 1799, was marked by significant events that reshaped France's political and social landscape. Key milestones include:\n",
       "\n",
       "- **1789**:\n",
       "  - **May 5**: The Estates-General convened by Louis XVI at Versailles to address the financial crisis .\n",
       "  - **June 17**: The Third Estate declared itself the National Assembly, challenging the existing political order .\n",
       "  - **July 14**: The Storming of the Bastille, symbolizing the fall of the Ancien Régime .\n",
       "  - **August 26**: The Declaration of the Rights of Man and of the Citizen was approved .\n",
       "\n",
       "- **1792**:\n",
       "  - **August 10**: The Tuileries Palace was stormed, leading to the downfall of the monarchy .\n",
       "  - **September**: The French First Republic was established, ending the monarchy .\n",
       "\n",
       "- **1793**:\n",
       "  - **January 21**: Execution of King Louis XVI, marking a turning point in the Revolution .\n",
       "  - **April 6**: The Committee of Public Safety was created, leading to the Reign of Terror .\n",
       "  - **March-July**: The Reign of Terror, characterized by extreme political repression and mass executions .\n",
       "\n",
       "- **1794**:\n",
       "  - **July 28**: Execution of Robespierre, ending the Reign of Terror .\n",
       "\n",
       "- **1799**:\n",
       "  - **November 9**: The Coup of 18 Brumaire led by Napoleon Bonaparte, establishing the French Consulate and marking the end of the French Revolution .\n",
       "\n",
       "These events collectively highlight the radical transformation of French society and governance during the Revolution.\n",
       "\n",
       "---\n",
       "\n",
       "**Who was Robespierre and what was his role in the French revolution? (ask_embeddings)**\n",
       "\n",
       "### Maximilien Robespierre and His Role in the French Revolution\n",
       "\n",
       "Maximilien Robespierre was a prominent and influential figure during the French Revolution. His actions and ideologies significantly shaped the course of the revolution, particularly during its most radical phase.\n",
       "\n",
       "#### Key Points:\n",
       "- **Political Influence and Reforms**:\n",
       "  - Robespierre opposed the criteria for \"active citizens,\" advocating for broader political participation, which gained him substantial support among the Parisian populace .\n",
       "  - He played a pivotal role in the radical Jacobin club and was instrumental in pushing for universal male suffrage and other radical reforms through the new Constitution ratified on 24 June 1793 .\n",
       "\n",
       "- **Committee of Public Safety and the Reign of Terror**:\n",
       "  - Robespierre became a leading member of the Committee of Public Safety, which assumed control after the constitution was suspended in June 1793. He was a central figure during the Reign of Terror, a period marked by mass executions of perceived enemies of the revolution .\n",
       "  - Under his influence, the Law of Suspects was enacted, leading to the arrest and execution of thousands .\n",
       "\n",
       "- **Downfall and Execution**:\n",
       "  - Robespierre's dominance and the ensuing terror led to growing opposition. On 26 July 1794, he accused certain members of the Convention of conspiracy, which backfired and led to his arrest. After a failed suicide attempt, he was executed on 28 July 1794, marking the end of the Reign of Terror .\n",
       "\n",
       "Robespierre's legacy remains controversial; he is seen as both a defender of revolutionary ideals and a symbol of the revolution's excesses. His execution was a turning point that ended the most extreme phase of the French Revolution.\n",
       "\n",
       "---\n",
       "\n",
       "**Who was Robespierre and what was his role in the French revolution? (ask_graph)**\n",
       "\n",
       "### Maximilien Robespierre: Key Figure in the French Revolution\n",
       "\n",
       "Maximilien Robespierre was a central figure in the French Revolution, known for his radical leadership and controversial actions. His influence spanned various political factions and revolutionary activities, significantly shaping the course of the revolution.\n",
       "\n",
       "#### Key Roles and Actions:\n",
       "\n",
       "- **Leadership During the Reign of Terror**:\n",
       "  - Robespierre's leadership during the Reign of Terror was marked by extreme political repression and mass executions, contributing to an atmosphere of fear and instability in Paris .\n",
       "  - His actions and policies led to accusations of dictatorship, ultimately resulting in his execution on 28 July 1794, which marked the end of the Reign of Terror .\n",
       "\n",
       "- **Advocacy for the Third Estate**:\n",
       "  - He was deeply involved in advocating for the Third Estate, organizing meetings, petitions, and literature to support their cause .\n",
       "  - His involvement in the Estates-General underscored the growing influence of the common people in the political process .\n",
       "\n",
       "- **Association with Radical Factions**:\n",
       "  - Robespierre was a leading figure of the Montagnards, a radical political faction, and was closely associated with the Jacobins, Cordeliers, and the Society of Thirty .\n",
       "  - He opposed both moderate and radical factions within the Montagnard group, reflecting the complex political dynamics of the time .\n",
       "\n",
       "- **Influence on Revolutionary Policies**:\n",
       "  - He proposed significant motions in the Legislative Assembly, such as barring existing deputies from elections, which had substantial political implications .\n",
       "  - Robespierre led the Cult of the Supreme Being, a revolutionary cult, although it faced opposition and ridicule, contributing to his downfall .\n",
       "\n",
       "Robespierre's actions and policies were pivotal in shaping the revolutionary activities and the political landscape of France during this tumultuous period .\n",
       "\n",
       "---\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "md = \"\"\n",
    "\n",
    "for question in [\n",
    "  'Timeline of the French revolution',\n",
    "  'Who was Robespierre and what was his role in the French revolution?',\n",
    "  ]:\n",
    "  for func in [ask_embeddings, ask_graph]:\n",
    "    result = func(question)\n",
    "    md += f\"**{question} ({func.__name__})**\\n\\n{result}\\n\\n---\\n\\n\"\n",
    "\n",
    "Markdown(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial results look quite good. For a more comprehensive evaluation, let's use our LLM to judge between attempts by both techniques to answer the same question. When making a judgement call, the LLM doesn't have access to the original text, but we can assume that it has been exposed to Wikipedia and many other sources in pre-training and can evaluate the accuracy and relevance of a question. We'll run each of our 5 questions 5 times with each method, and look at the result to decide who won the battle of the semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How did the financial and political crisis contribute to the calling of the Estates-General in 1789?\n",
      "Winner: graph\n",
      "Graph wins: 5, Embeddings wins: 0\n",
      "Explanations:\n",
      "  1 (graph won). The best answer is the first one because it acknowledges the inability to answer the question directly, whereas the other answer is irrelevant and does not address the question at all.\n",
      "  2 (graph won). The best answer is the other answer because it directly addresses the inability to answer the question based on the provided data, whereas the other answer is irrelevant and does not address the question at all.\n",
      "  3 (graph won). The best answer is the second one because it acknowledges the inability to answer the question based on the provided data. The other answer does not address the question at all and instead asks for the question to be provided again.\n",
      "  4 (graph won). The best answer is the first one because it directly acknowledges the inability to answer the question, while the other answer is irrelevant and does not address the question at all.\n",
      "  5 (graph won). Answer 1 is the best answer because it acknowledges the inability to answer the question directly, which is more relevant to the user's query. Answer 2 is less relevant as it asks for a specific question related to a dataset, which is not pertinent to the user's original question about the Estates-General in 1789.\n",
      "\n",
      "Question: What role did the Enlightenment and previous revolutions play in shaping the French Revolution?\n",
      "Winner: graph\n",
      "Graph wins: 3, Embeddings wins: 2\n",
      "Explanations:\n",
      "  1 (graph won). The best answer is more relevant as it directly addresses the inability to answer the question, whereas the other answer asks for a question that has already been provided.\n",
      "  2 (graph won). The best answer is the second one as it directly addresses the user's question by acknowledging the inability to answer based on the provided data. The first answer is irrelevant and does not address the question at all.\n",
      "  3 (graph won). The best answer is more relevant to the question as it acknowledges the inability to provide an answer, while the other answer is completely irrelevant and asks for a question about a dataset, which is not related to the user's query.\n",
      "  4 (embeddings won). Answer 1 acknowledges the lack of a specific question, which is accurate given the context. Answer 2 incorrectly claims an inability to answer based on provided data, which is irrelevant.\n",
      "  5 (embeddings won). Answer 1 is the best answer because it attempts to engage with the question, while Answer 2 simply states an inability to answer without offering any relevant information.\n",
      "\n",
      "Question: Analyze how the various social classes in France were affected by the Revolution and the policies implemented, such as the Civil Constitution of the Clergy and the abolition of feudal dues.\n",
      "Winner: graph\n",
      "Graph wins: 4, Embeddings wins: 1\n",
      "Explanations:\n",
      "  1 (graph won). The best answer is answer 1 because it acknowledges the inability to answer the question based on the provided data. The other answer does not address the question and instead asks for a specific question about a dataset, which is irrelevant.\n",
      "  2 (graph won). The best answer is more relevant as it acknowledges the inability to answer the question based on the data provided. The other answer does not address the question and instead asks for a different question, which is not helpful.\n",
      "  3 (graph won). The best answer is the second one because it directly addresses the inability to answer the question based on the provided data. The other answer does not address the question at all and instead asks for a question to be provided.\n",
      "  4 (embeddings won). Answer 1 asks for clarification on the question, showing an attempt to engage with the topic. Answer 2 directly states an inability to answer, providing no value or relevance.\n",
      "  5 (graph won). The best answer is answer 2 because it acknowledges the inability to answer the question based on the provided data, whereas answer 1 is irrelevant to the question asked.\n",
      "\n",
      "Question: What were the key events of the French Revolution that led to the rise of Napoleon Bonaparte?\n",
      "Winner: graph\n",
      "Graph wins: 5, Embeddings wins: 0\n",
      "Explanations:\n",
      "  1 (graph won). The best answer is more accurate and relevant as it directly addresses the inability to answer the question given the data. The other answer incorrectly assumes the question is incomplete and requests more details about a dataset, which is not mentioned in the user's question.\n",
      "  2 (graph won). The best answer is the second one because it acknowledges the inability to answer given the data, which is more accurate and relevant to the question. The other answer is completely off-topic and does not address the question at all.\n",
      "  3 (graph won). The best answer is the first one because it directly addresses the inability to answer the question with the provided data, while the other answer asks for more context without addressing the question at all.\n",
      "  4 (graph won). The best answer is more relevant as it acknowledges the inability to answer the question directly. The other answer incorrectly assumes the question is related to a dataset and asks for clarification, which is not relevant.\n",
      "  5 (graph won). Answer 1 directly addresses the inability to answer the question, while Answer 2 is irrelevant as it asks for more specific details about a dataset that isn't mentioned in the question.\n",
      "\n",
      "Question: The role of ideology in the French Revolution is a subject of ongoing debate among historians. Analyze the conflicting interpretations of Jonathan Israel and Alfred Cobban, assessing their arguments and evidence in light of the information presented in the text.\n",
      "Winner: graph\n",
      "Graph wins: 4, Embeddings wins: 1\n",
      "Explanations:\n",
      "  1 (graph won). The best answer is the second one because it acknowledges the inability to answer the question based on the provided data. The other answer does not address the question at all and seems irrelevant.\n",
      "  2 (embeddings won). Answer 2 is the best answer as it attempts to seek clarification from the user, showing a willingness to engage with the question. The other answer simply states an inability to answer, providing no further interaction or attempt to resolve the issue.\n",
      "  3 (graph won). The best answer is the second one because it acknowledges the inability to answer the question based on the provided data, which is accurate and relevant. The other answer does not address the question at all.\n",
      "  4 (graph won). The best answer is the second one because it acknowledges the inability to answer the question given the data. The first answer is irrelevant and does not address the question at all.\n",
      "  5 (graph won). The best answer is more accurate and relevant as it acknowledges the lack of provided data to answer the question. The other answer requests the question again, which is unnecessary since the question is already given.\n",
      "\n",
      "-----------------------\n",
      "Total Graph Wins: 21\n",
      "Total Embeddings Wins: 4\n",
      "Total Winner: graph\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "import instructor\n",
    "import json\n",
    "import random\n",
    "\n",
    "QUESTIONS = [\n",
    "  'How did the financial and political crisis contribute to the calling of the Estates-General in 1789?',\n",
    "  'What role did the Enlightenment and previous revolutions play in shaping the French Revolution?',\n",
    "  'Analyze how the various social classes in France were affected by the Revolution and the policies implemented, such as the Civil Constitution of the Clergy and the abolition of feudal dues.',\n",
    "  'What were the key events of the French Revolution that led to the rise of Napoleon Bonaparte?',\n",
    "  'The role of ideology in the French Revolution is a subject of ongoing debate among historians. Analyze the conflicting interpretations of Jonathan Israel and Alfred Cobban, assessing their arguments and evidence in light of the information presented in the text.',\n",
    "]\n",
    "\n",
    "class EvalAnswers(BaseModel):\n",
    "  best_answer: Literal[1, 2] = Field(..., description=\"The index of the best answer, evaluated for accuracy and relevance\")\n",
    "  explanation: str = Field(..., description=(\n",
    "    \"Short explanation for the choice of the best answer (max 100 tokens). \"\n",
    "    \"The explanation refers to the chosen best answer as 'the best answer' \"\n",
    "    \"and the other answer as 'the other answer'.\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "evals = []\n",
    "\n",
    "for question in QUESTIONS:\n",
    "  evals_best_answer = []\n",
    "  evals_explanation = []\n",
    "  for i in range(5):\n",
    "    answer_graph = ask_graph('question')\n",
    "    answer_embeddings = ask_embeddings('question')\n",
    "    graph_index = random.choice([1, 2])\n",
    "    embeddings_index = 1 if graph_index == 2 else 2\n",
    "    evaluation = instructor.from_openai(oai).chat.completions.create(\n",
    "      response_model=EvalAnswers,\n",
    "      model=GPT_4_O_MODEL_NAME,\n",
    "      messages=[\n",
    "        {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": (\"Evaluate the two answers below based on accuracy and relevance to the question. \"\n",
    "                      \"Select the index of the best answer (1 or 2) and explain why you made that choice.\")\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": json.dumps({\n",
    "          'question': question,\n",
    "          'answers': dict(sorted({\n",
    "            graph_index: answer_graph,\n",
    "            embeddings_index: answer_embeddings,\n",
    "          }.items())),\n",
    "        })},\n",
    "      ],\n",
    "      max_tokens=250,\n",
    "      temperature=0.5,\n",
    "    )\n",
    "    evals_best_answer.append('graph' if evaluation.best_answer == graph_index else 'embeddings')\n",
    "    evals_explanation.append(evaluation.explanation)\n",
    "  embeddings_wins = evals_best_answer.count('embeddings')\n",
    "  graph_wins = evals_best_answer.count('graph')\n",
    "  evals.append({\n",
    "    'question': question,\n",
    "    'best_answer': evals_best_answer,\n",
    "    'explanation': evals_explanation,\n",
    "    'graph_wins': graph_wins,\n",
    "    'embeddings_wins': embeddings_wins,\n",
    "    'winner': 'graph' if graph_wins > embeddings_wins else 'embeddings',\n",
    "  })\n",
    "  total_graph_wins = sum([ev['graph_wins'] for ev in evals])\n",
    "  total_embeddings_wins = sum([ev['embeddings_wins'] for ev in evals])\n",
    "  total_winner = 'graph' if total_graph_wins > total_embeddings_wins else 'embeddings'\n",
    "\n",
    "for ev in evals:\n",
    "  print(f\"Question: {ev['question']}\")\n",
    "  print(f\"Winner: {ev['winner']}\")\n",
    "  print(f\"Graph wins: {ev['graph_wins']}, Embeddings wins: {ev['embeddings_wins']}\")\n",
    "  print(\"Explanations:\")\n",
    "  for i, explanation in enumerate(ev['explanation']):\n",
    "    print(f\"  {i+1} ({ev['best_answer'][i]} won). {explanation}\")\n",
    "  print()\n",
    "print(\"-----------------------\")\n",
    "print(f\"Total Graph Wins: {total_graph_wins}\")\n",
    "print(f\"Total Embeddings Wins: {total_embeddings_wins}\")\n",
    "print(f\"Total Winner: {total_winner}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So GraphRag is the clear winner. No wonder - by analysing the text, extracting entities and facts, and retrieving them to construct a rich context, it prepares the LLM to answer our questions with depth and accuracy. The cost upfront (in both time and money) is significantly higher, but at inference time the expense, while still higher, is tolerable, and similar to many complex retrieval systems, while resulting in a much better understanding of the data and superior result quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
